import os
from pathlib import Path
from dotenv import load_dotenv
from evaluateur_reponse import evaluer_possibilite_reponse
from rag_transcripts import TranscriptRAG
from mistralai import Mistral
import requests
import torch
import re

# Chargement des variables d'environnement
load_dotenv()

# Liste de mots-cl√©s qui indiquent une question conversationnelle
QUESTIONS_CONVERSATIONNELLES = [
    r"\bbonjour\b", r"\bsalut\b", r"\bcoucou\b", r"\bhello\b", r"\bcc\b",
    r"\b√ßa va\b", r"\bca va\b", r"\bcomment vas[ -]tu\b", r"\btu vas bien\b",
    r"\bau revoir\b", r"\badieu\b", r"\bmerci\b", r"\b√† plus\b", r"\ba plus\b",
    r"\bbonsoir\b"
]

def est_question_conversationnelle(question: str) -> bool:
    """
    D√©termine si la question est une salutation ou une question conversationnelle
    plut√¥t qu'une recherche d'information.
    
    Args:
        question: La question pos√©e
        
    Returns:
        bool: True si c'est une question conversationnelle
    """
    question = question.lower()
    
    # V√©rifier les patterns conversationnels
    for pattern in QUESTIONS_CONVERSATIONNELLES:
        if re.search(pattern, question):
            return True
    
    # V√©rifier si la question est tr√®s courte (probablement conversationnelle)
    if len(question.split()) <= 3 and ("?" in question or not any(c.isalpha() for c in question)):
        return True
    
    return False

def generer_reponse_directe(question: str, mode: str = "api", temperature: float = 0.3, max_tokens: int = 150):
    """
    G√©n√®re une r√©ponse directe sans v√©rification de document, pour les cas o√π
    les documents ne contiennent pas d'information pertinente.
    
    Args:
        question: La question pos√©e par l'utilisateur
        mode: Mode d'appel du mod√®le ("api" ou "local")
        
    Returns:
        str: La r√©ponse g√©n√©r√©e
    """
    # Prompt simple pour r√©pondre √† des questions g√©n√©rales
    prompt = """Vous √™tes un assistant conversationnel utile et amical.
R√©pondez de mani√®re br√®ve et directe √† la question pos√©e.
Limitez votre r√©ponse √† 2-3 phrases maximum.
Si la question est une salutation ou une question de routine, r√©pondez de fa√ßon naturelle et amicale."""
    
    # Pr√©parer les messages
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": question}
    ]
    
    # Appeler le mod√®le selon le mode choisi
    if mode == "api":
        try:
            api_key = os.environ["MISTRAL_API_KEY"]
            client = Mistral(api_key=api_key)
            model = "mistral-small-latest"
            
            chat_response = client.chat.complete(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            return chat_response.choices[0].message.content.strip()
        except Exception as e:
            print(f"Erreur lors de l'appel √† l'API Mistral: {str(e)}")
            return "Bonjour! Comment puis-je vous aider aujourd'hui?"
    else:
        try:
            url = "http://localhost:11434/v1/chat/completions"
            payload = {
                "model": "Mistral-Large-Instruct-2407-AWQ",
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens
            }
            
            response = requests.post(url, json=payload)
            return response.json()["choices"][0]["message"]["content"].strip()
        except Exception as e:
            print(f"Erreur lors de l'appel au mod√®le local: {str(e)}")
            return "Bonjour! Comment puis-je vous aider aujourd'hui?"

def verifier_similarite_manuelle(question: str, seuil_similarite: float = 0.15):
    """
    V√©rifie manuellement si les chunks ont une similarit√© suffisante.
    Strat√©gie plus simple et plus directe que d'utiliser l'√©valuateur LLM.
    
    Args:
        question: La question pos√©e
        seuil_similarite: Seuil minimal de similarit√© pour consid√©rer un chunk pertinent
        
    Returns:
        bool: True si au moins un chunk d√©passe le seuil de similarit√©
    """
    # R√©cup√©rer les chunks pertinents
    rag = TranscriptRAG.get_instance()
    # Utiliser top_k=5 pour avoir suffisamment de chunks √† √©valuer
    relevant_chunks = rag.search(question, top_k=5)
    
    # V√©rifier si au moins un chunk d√©passe le seuil de similarit√©
    for chunk, similarite in relevant_chunks:
        if similarite > seuil_similarite:
            return True, relevant_chunks
    
    return False, relevant_chunks

def assistant_intelligent(question: str, mode: str = "api", seuil_similarite: float = 0.15):
    """
    Assistant qui r√©pond aux questions en utilisant diff√©rentes strat√©gies selon
    la pertinence des documents.
    
    Args:
        question: La question pos√©e par l'utilisateur
        mode: Mode d'appel du mod√®le ("api" ou "local")
        seuil_similarite: Seuil minimal de similarit√© pour consid√©rer un document pertinent
        
    Returns:
        dict: R√©sultat contenant la r√©ponse et les m√©tadonn√©es
    """
    print(f"Question: {question}")
    
    # D√©tecter si c'est une question conversationnelle
    if est_question_conversationnelle(question):
        print("üó£Ô∏è Question conversationnelle d√©tect√©e, g√©n√©ration d'une r√©ponse directe...")
        reponse_texte = generer_reponse_directe(question, mode=mode)
        return {
            "reponse": reponse_texte,
            "type_reponse": "conversationnelle",
            "documents_pertinents": False
        }
    
    print("√âvaluation de la question...")
    
    # V√©rifier directement avec le seuil de similarit√© (plus fiable que l'√©valuateur LLM)
    peut_repondre, chunks = verifier_similarite_manuelle(question, seuil_similarite)
    
    # Afficher les similarit√©s pour debug
    print("Similarit√©s des chunks trouv√©s:")
    for i, (chunk, similarite) in enumerate(chunks[:3]):
        source = Path(chunk.source_file).name
        est_image = "IMAGE" if chunk.is_image_description else "TEXTE"
        print(f"  Chunk {i+1}: {similarite:.4f} ({est_image}, {source})")
    
    if peut_repondre:
        print(f"‚úÖ Documents pertinents trouv√©s (similarit√© > {seuil_similarite}), g√©n√©ration d'une r√©ponse d√©taill√©e...")
        # Utiliser le RAG complet pour une r√©ponse d√©taill√©e
        rag = TranscriptRAG.get_instance()
        reponse_texte = rag.query(question)
        
        return {
            "reponse": reponse_texte,
            "type_reponse": "d√©taill√©e",
            "documents_pertinents": True
        }
    else:
        print(f"‚ùå Pas de documents pertinents (similarit√© < {seuil_similarite}), g√©n√©ration d'une r√©ponse directe...")
        # Utiliser une r√©ponse directe sans passer par le processus RAG
        reponse_texte = generer_reponse_directe(question, mode=mode)
        
        return {
            "reponse": reponse_texte,
            "type_reponse": "directe",
            "documents_pertinents": False
        }

# Interface utilisateur simple pour tester
def interface_test():
    """Interface simple pour tester l'assistant."""
    
    # Initialiser le RAG
    print("Initialisation du syst√®me RAG...")
    rag = TranscriptRAG.get_instance("/Users/rekta/projet/backend/transcripts")
    print("Syst√®me pr√™t!")
    
    # Demander si on veut utiliser l'API ou local
    mode = input("Mode d'ex√©cution (api/local, d√©faut: api): ").strip().lower()
    if mode != "local":
        mode = "api"
    
    # Demander le seuil de similarit√©
    seuil_input = input("Seuil de similarit√© (d√©faut: 0.15): ").strip()
    try:
        seuil_similarite = float(seuil_input) if seuil_input else 0.15
    except ValueError:
        print("Valeur invalide, utilisation du seuil par d√©faut: 0.15")
        seuil_similarite = 0.15
    
    print(f"Mode s√©lectionn√©: {mode}")
    print(f"Seuil de similarit√©: {seuil_similarite}")
    print("-" * 50)
    
    # Boucle principale
    while True:
        question = input("\nVotre question (ou 'q' pour quitter): ").strip()
        
        if question.lower() in ['q', 'quit', 'exit']:
            break
        
        print("-" * 50)
        resultat = assistant_intelligent(question, mode=mode, seuil_similarite=seuil_similarite)
        
        print("\nR√©ponse:")
        print(resultat["reponse"])
        print(f"\nType de r√©ponse: {resultat['type_reponse']}")
        print("-" * 50)

if __name__ == "__main__":
    interface_test() 